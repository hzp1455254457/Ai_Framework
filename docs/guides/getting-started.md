# å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ“‹ æ–‡æ¡£è¯´æ˜

æœ¬æ–‡æ¡£å¸®åŠ©æ–°ç”¨æˆ·å¿«é€Ÿä¸Šæ‰‹AIæ¡†æ¶ï¼Œä»å®‰è£…åˆ°è¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹ï¼Œåªéœ€å‡ åˆ†é’Ÿã€‚

**é¢„è®¡æ—¶é—´**: 10-15åˆ†é’Ÿ

---

## ğŸ“¦ ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡

### 1.1 æ£€æŸ¥Pythonç‰ˆæœ¬

AIæ¡†æ¶éœ€è¦ **Python 3.10+**ï¼ˆæ¨è 3.11+ï¼‰ã€‚

```bash
# æ£€æŸ¥Pythonç‰ˆæœ¬
python --version
# æˆ–
python3 --version
```

å¦‚æœç‰ˆæœ¬ä½äº3.10ï¼Œè¯·å…ˆå‡çº§Pythonã€‚

### 1.2 åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate
```

---

## ğŸ”§ ç¬¬äºŒæ­¥ï¼šå®‰è£…æ¡†æ¶

### 2.1 å…‹éš†é¡¹ç›®ï¼ˆå¦‚æœä»Gitä»“åº“ï¼‰

```bash
git clone <repository-url>
cd Ai_Framework
```

### 2.2 å®‰è£…ä¾èµ–

```bash
# å®‰è£…ç”Ÿäº§ä¾èµ–
pip install -r requirements.txt

# å®‰è£…å¼€å‘ä¾èµ–ï¼ˆå¯é€‰ï¼Œç”¨äºå¼€å‘å’Œæµ‹è¯•ï¼‰
pip install -r requirements-dev.txt
```

### 2.3 éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥å…³é”®æ¨¡å—æ˜¯å¦å¯ä»¥å¯¼å…¥
python -c "from core.llm.service import LLMService; print('å®‰è£…æˆåŠŸï¼')"
```

---

## âš™ï¸ ç¬¬ä¸‰æ­¥ï¼šé…ç½®APIå¯†é’¥

### 3.1 è·å–APIå¯†é’¥

æ ¹æ®ä½ è¦ä½¿ç”¨çš„LLMæä¾›å•†ï¼Œè·å–å¯¹åº”çš„APIå¯†é’¥ï¼š

- **é€šä¹‰åƒé—®**: è®¿é—® [é˜¿é‡Œäº‘DashScope](https://dashscope.console.aliyun.com/) è·å–APIå¯†é’¥
- **OpenAI**: è®¿é—® [OpenAI Platform](https://platform.openai.com/) è·å–APIå¯†é’¥
- **DeepSeek**: è®¿é—® [DeepSeek Platform](https://platform.deepseek.com/) è·å–APIå¯†é’¥
- **Claude**: è®¿é—® [Anthropic Console](https://console.anthropic.com/) è·å–APIå¯†é’¥

### 3.2 é…ç½®APIå¯†é’¥

**æ–¹å¼1ï¼šä¿®æ”¹é…ç½®æ–‡ä»¶**ï¼ˆæ¨èå¼€å‘ç¯å¢ƒï¼‰

ç¼–è¾‘ `config/default.yaml` æˆ– `config/dev.yaml`ï¼š

```yaml
llm:
  adapters:
    qwen-adapter:
      api_key: "ä½ çš„APIå¯†é’¥"
    openai-adapter:
      api_key: "ä½ çš„OpenAI APIå¯†é’¥"
```

**æ–¹å¼2ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡**ï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰

```bash
# Windows PowerShell
$env:QWEN_API_KEY="ä½ çš„APIå¯†é’¥"
$env:OPENAI_API_KEY="ä½ çš„OpenAI APIå¯†é’¥"

# Linux/Mac
export QWEN_API_KEY="ä½ çš„APIå¯†é’¥"
export OPENAI_API_KEY="ä½ çš„OpenAI APIå¯†é’¥"
```

---

## ğŸš€ ç¬¬å››æ­¥ï¼šè¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹

### 4.1 åŸºç¡€èŠå¤©ç¤ºä¾‹

åˆ›å»ºæ–‡ä»¶ `my_first_chat.py`ï¼š

```python
import asyncio
from infrastructure.config import ConfigManager
from core.llm.service import LLMService

async def main():
    # 1. åŠ è½½é…ç½®
    config_manager = ConfigManager.load(env="dev")
    config = config_manager.get_all()
    
    # 2. åˆ›å»ºLLMæœåŠ¡
    service = LLMService(config)
    await service.initialize()
    
    # 3. å‘é€æ¶ˆæ¯
    messages = [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}]
    response = await service.chat(
        messages=messages,
        model="qwen-turbo"  # æˆ–ä½ é…ç½®çš„å…¶ä»–æ¨¡å‹
    )
    
    # 4. æ˜¾ç¤ºç»“æœ
    print(f"AIå›å¤: {response.content}")
    print(f"Tokenä½¿ç”¨: {response.total_tokens}")
    
    # 5. æ¸…ç†èµ„æº
    await service.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

è¿è¡Œç¤ºä¾‹ï¼š

```bash
python my_first_chat.py
```

**é¢„æœŸè¾“å‡º**:
```
AIå›å¤: ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹...
Tokenä½¿ç”¨: 30
```

### 4.2 ä½¿ç”¨é¡¹ç›®ç¤ºä¾‹

é¡¹ç›®æ ¹ç›®å½•æä¾›äº† `simple_chat_example.py` ç¤ºä¾‹ï¼š

```bash
python simple_chat_example.py
```

---

## ğŸŒ ç¬¬äº”æ­¥ï¼šå¯åŠ¨APIæœåŠ¡ï¼ˆå¯é€‰ï¼‰

å¦‚æœä½ æƒ³é€šè¿‡HTTP APIä½¿ç”¨æ¡†æ¶ï¼š

### 5.1 å¯åŠ¨FastAPIæœåŠ¡

```bash
# æ–¹å¼1ï¼šä½¿ç”¨uvicornç›´æ¥å¯åŠ¨
uvicorn api.fastapi_app:app --reload --host 0.0.0.0 --port 8000

# æ–¹å¼2ï¼šä½¿ç”¨Pythonæ¨¡å—æ–¹å¼
python -m uvicorn api.fastapi_app:app --reload
```

### 5.2 è®¿é—®APIæ–‡æ¡£

å¯åŠ¨æœåŠ¡åï¼Œè®¿é—®ä»¥ä¸‹åœ°å€ï¼š

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **OpenAPI JSON**: http://localhost:8000/openapi.json

### 5.3 æµ‹è¯•API

```bash
# æµ‹è¯•å¥åº·æ£€æŸ¥
curl http://localhost:8000/api/v1/health/

# æµ‹è¯•èŠå¤©æ¥å£
curl -X POST "http://localhost:8000/api/v1/llm/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "model": "qwen-turbo"
  }'
```

---

## ğŸ’¡ å¸¸è§ä½¿ç”¨åœºæ™¯

### åœºæ™¯1ï¼šç®€å•é—®ç­”

```python
import asyncio
from infrastructure.config import ConfigManager
from core.llm.service import LLMService

async def simple_qa():
    config = ConfigManager.load(env="dev").get_all()
    service = LLMService(config)
    await service.initialize()
    
    response = await service.chat(
        messages=[{"role": "user", "content": "Pythonä¸­å¦‚ä½•è¯»å–æ–‡ä»¶ï¼Ÿ"}],
        model="qwen-turbo"
    )
    
    print(response.content)
    await service.cleanup()

asyncio.run(simple_qa())
```

### åœºæ™¯2ï¼šå¤šè½®å¯¹è¯

```python
import asyncio
from infrastructure.config import ConfigManager
from core.llm.service import LLMService
from core.llm.context import ConversationContext

async def multi_turn_chat():
    config = ConfigManager.load(env="dev").get_all()
    service = LLMService(config)
    await service.initialize()
    
    # åˆ›å»ºå¯¹è¯ä¸Šä¸‹æ–‡
    context = ConversationContext()
    
    # ç¬¬ä¸€è½®
    context.add_user_message("æˆ‘æƒ³å­¦ä¹ Python")
    response1 = await service.chat_with_context(context)
    print(f"AI: {response1.content}")
    
    # ç¬¬äºŒè½®
    context.add_user_message("æ¨èä¸€äº›å­¦ä¹ èµ„æº")
    response2 = await service.chat_with_context(context)
    print(f"AI: {response2.content}")
    
    await service.cleanup()

asyncio.run(multi_turn_chat())
```

### åœºæ™¯3ï¼šAgentä»»åŠ¡æ‰§è¡Œ

```python
import asyncio
from infrastructure.config import ConfigManager
from core.agent.engine import AgentEngine

async def agent_task():
    config = ConfigManager.load(env="dev").get_all()
    engine = AgentEngine(config)
    await engine.initialize()
    
    # æ‰§è¡ŒAgentä»»åŠ¡
    result = await engine.run_task(
        task="æŸ¥è¯¢åŒ—äº¬å¤©æ°”ï¼Œç„¶åå‘Šè¯‰æˆ‘é€‚åˆç©¿ä»€ä¹ˆè¡£æœ",
        model="gpt-3.5-turbo"
    )
    
    print(f"ä»»åŠ¡ç»“æœ: {result['content']}")
    print(f"å·¥å…·è°ƒç”¨: {result.get('tool_calls', [])}")
    
    await engine.cleanup()

asyncio.run(agent_task())
```

### åœºæ™¯4ï¼šæµå¼è¾“å‡º

```python
import asyncio
from infrastructure.config import ConfigManager
from core.llm.service import LLMService

async def stream_chat():
    config = ConfigManager.load(env="dev").get_all()
    service = LLMService(config)
    await service.initialize()
    
    messages = [{"role": "user", "content": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"}]
    
    print("AIå›å¤: ", end="", flush=True)
    async for chunk in service.stream_chat(
        messages=messages,
        model="qwen-turbo"
    ):
        print(chunk.content, end="", flush=True)
    print()  # æ¢è¡Œ
    
    await service.cleanup()

asyncio.run(stream_chat())
```

---

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜1ï¼šModuleNotFoundError

**ç—‡çŠ¶**: å¯¼å…¥æ¨¡å—æ—¶æç¤ºæ‰¾ä¸åˆ°æ¨¡å—

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
cd Ai_Framework

# ç¡®ä¿è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»
# é‡æ–°å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### é—®é¢˜2ï¼šAPIå¯†é’¥é”™è¯¯

**ç—‡çŠ¶**: è°ƒç”¨LLMæ—¶æç¤ºè®¤è¯å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„APIå¯†é’¥æ˜¯å¦æ­£ç¡®
2. æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦è®¾ç½®
3. éªŒè¯APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆï¼ˆè®¿é—®å¯¹åº”æä¾›å•†çš„å¹³å°ï¼‰

### é—®é¢˜3ï¼šæ¨¡å‹ä¸å­˜åœ¨

**ç—‡çŠ¶**: æç¤ºæ¨¡å‹ä¸å­˜åœ¨æˆ–æœªæ‰¾åˆ°é€‚é…å™¨

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰
2. è¿è¡Œ `GET /api/v1/llm/models` æŸ¥çœ‹æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
3. æ£€æŸ¥é€‚é…å™¨æ˜¯å¦å·²æ­£ç¡®æ³¨å†Œ

### é—®é¢˜4ï¼šç«¯å£è¢«å ç”¨

**ç—‡çŠ¶**: å¯åŠ¨APIæœåŠ¡æ—¶æç¤ºç«¯å£è¢«å ç”¨

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨å…¶ä»–ç«¯å£
uvicorn api.fastapi_app:app --reload --port 8001

# æˆ–åœæ­¢å ç”¨ç«¯å£çš„è¿›ç¨‹
# Windows:
netstat -ano | findstr :8000
taskkill /PID <è¿›ç¨‹ID> /F
```

---

## ğŸ“š ä¸‹ä¸€æ­¥å­¦ä¹ 

å®Œæˆå¿«é€Ÿå¼€å§‹åï¼Œå»ºè®®æŒ‰ä»¥ä¸‹é¡ºåºæ·±å…¥å­¦ä¹ ï¼š

### 1. äº†è§£æ ¸å¿ƒæ¦‚å¿µ

- [æ¶æ„æ–¹æ¡ˆæ–‡æ¡£](../../AIæ¡†æ¶æ¶æ„æ–¹æ¡ˆæ–‡æ¡£.md) - ç†è§£æ•´ä½“æ¶æ„
- [APIå‚è€ƒæ–‡æ¡£](../api/api-reference.md) - äº†è§£æ‰€æœ‰APIæ¥å£

### 2. å­¦ä¹ é«˜çº§åŠŸèƒ½

- **Agentå¼•æ“**: å­¦ä¹ å¦‚ä½•ä½¿ç”¨Agentæ‰§è¡Œå¤æ‚ä»»åŠ¡
- **å·¥å…·ç³»ç»Ÿ**: å­¦ä¹ å¦‚ä½•å®šä¹‰å’Œä½¿ç”¨å·¥å…·
- **è®°å¿†ç®¡ç†**: å­¦ä¹ å¦‚ä½•ä½¿ç”¨çŸ­æœŸå’Œé•¿æœŸè®°å¿†
- **ä»»åŠ¡è§„åˆ’å™¨**: å­¦ä¹ å¦‚ä½•ä½¿ç”¨è§„åˆ’å™¨åˆ†è§£å¤æ‚ä»»åŠ¡

### 3. æŸ¥çœ‹ç¤ºä¾‹ä»£ç 

- `simple_chat_example.py` - åŸºç¡€èŠå¤©ç¤ºä¾‹
- `examples/` ç›®å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰- æ›´å¤šç¤ºä¾‹ä»£ç 

### 4. é˜…è¯»è®¾è®¡æ–‡æ¡£

- `docs/design/` - å„æ¨¡å—çš„è¯¦ç»†è®¾è®¡æ–‡æ¡£
- `docs/architecture/` - æ¶æ„å†³ç­–å’Œä¾èµ–å…³ç³»

---

## ğŸ¯ å¿«é€Ÿå‚è€ƒ

### å¸¸ç”¨å‘½ä»¤

```bash
# å¯åŠ¨APIæœåŠ¡
uvicorn api.fastapi_app:app --reload

# è¿è¡Œæµ‹è¯•
pytest

# æŸ¥çœ‹APIæ–‡æ¡£
# è®¿é—® http://localhost:8000/docs
```

### é…ç½®æ–‡ä»¶ä½ç½®

- `config/default.yaml` - é»˜è®¤é…ç½®
- `config/dev.yaml` - å¼€å‘ç¯å¢ƒé…ç½®
- `config/prod.yaml` - ç”Ÿäº§ç¯å¢ƒé…ç½®

### é‡è¦æ–‡ä»¶

- `simple_chat_example.py` - ç®€å•èŠå¤©ç¤ºä¾‹
- `docs/api/api-reference.md` - APIå‚è€ƒæ–‡æ¡£
- `docs/guides/quick-reference.md` - å¿«é€Ÿå‚è€ƒæŒ‡å—

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [APIå‚è€ƒæ–‡æ¡£](../api/api-reference.md) - å®Œæ•´çš„APIæ¥å£è¯´æ˜
- [å¿«é€Ÿå‚è€ƒæŒ‡å—](quick-reference.md) - å¸¸ç”¨å‘½ä»¤å’Œä»£ç ç‰‡æ®µ
- [æ¶æ„æ–¹æ¡ˆæ–‡æ¡£](../../AIæ¡†æ¶æ¶æ„æ–¹æ¡ˆæ–‡æ¡£.md) - æ¶æ„è®¾è®¡å‚è€ƒ
- [é¡¹ç›®è®¡åˆ’æ–‡æ¡£](../../docs/PROJECT_PLAN.md) - é¡¹ç›®è¿›åº¦å’ŒåŠŸèƒ½æ¸…å•

---

## ğŸ”„ æ–‡æ¡£æ›´æ–°è®°å½•

| æ—¥æœŸ | ç‰ˆæœ¬ | æ›´æ–°å†…å®¹ | æ›´æ–°äºº |
|------|------|---------|--------|
| 2026-01-22 | v1.0 | åˆå§‹ç‰ˆæœ¬ï¼Œåˆ›å»ºå¿«é€Ÿå¼€å§‹æŒ‡å— | - |

---

**æç¤º**: å¦‚æœåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹[æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)éƒ¨åˆ†æˆ–æŸ¥é˜…ç›¸å…³æ–‡æ¡£ã€‚
