"""
æµ‹è¯•æ¨¡å—ï¼šæ€§èƒ½åŸºå‡†æµ‹è¯•
åŠŸèƒ½æè¿°ï¼šæµ‹è¯•æ–°æ¶æ„çš„æ€§èƒ½ï¼Œç¡®ä¿æ€§èƒ½ä¸ä¸‹é™
"""

import pytest
import asyncio
import time
from unittest.mock import AsyncMock, MagicMock, patch
from core.llm.service import LLMService
from core.llm.adapters.base import BaseLLMAdapter


class MockAdapter(BaseLLMAdapter):
    """ç”¨äºæ€§èƒ½æµ‹è¯•çš„Mocké€‚é…å™¨"""
    
    def __init__(self, name="mock-adapter", delay=0.01):
        super().__init__()
        self._name = name
        self._delay = delay  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
    
    @property
    def name(self) -> str:
        return self._name
    
    @property
    def provider(self) -> str:
        return "mock"
    
    async def call(self, messages, model, **kwargs):
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await asyncio.sleep(self._delay)
        return {
            "content": "Mock response",
            "usage": {"total_tokens": 10},
            "metadata": {},
        }
    
    async def stream_call(self, messages, model, **kwargs):
        # æ¨¡æ‹Ÿæµå¼å“åº”
        await asyncio.sleep(self._delay)
        yield {"content": "Mock", "usage": {"total_tokens": 5}}
        await asyncio.sleep(self._delay)
        yield {"content": " response", "usage": {"total_tokens": 5}}


@pytest.mark.asyncio
@pytest.mark.slow
class TestPerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    @pytest.fixture
    def service_config(self):
        """åˆ›å»ºæœåŠ¡é…ç½®fixture"""
        return {
            "llm": {
                "default_model": "test-model",
                "performance": {
                    "enable_connection_pool": True,
                    "enable_cache": True,
                    "enable_deduplication": True,
                },
            },
        }
    
    @pytest.fixture
    def service_config_no_optimization(self):
        """åˆ›å»ºæ— ä¼˜åŒ–é…ç½®fixture"""
        return {
            "llm": {
                "default_model": "test-model",
                "performance": {
                    "enable_connection_pool": False,
                    "enable_cache": False,
                    "enable_deduplication": False,
                },
            },
        }
    
    async def test_chat_latency_with_optimization(self, service_config):
        """æµ‹è¯•å¯ç”¨ä¼˜åŒ–åçš„èŠå¤©å»¶è¿Ÿ"""
        # Arrange
        service = LLMService(service_config)
        await service.initialize()
        
        adapter = MockAdapter(delay=0.01)
        service.register_adapter(adapter)
        
        messages = [{"role": "user", "content": "Hello"}]
        
        # Act - æ‰§è¡Œå¤šæ¬¡è¯·æ±‚å¹¶æµ‹é‡å»¶è¿Ÿ
        latencies = []
        for _ in range(10):
            start_time = time.time()
            await service.chat(messages)
            latency = time.time() - start_time
            latencies.append(latency)
        
        # Assert
        avg_latency = sum(latencies) / len(latencies)
        max_latency = max(latencies)
        
        # å¹³å‡å»¶è¿Ÿåº”è¯¥å°äº50msï¼ˆåŒ…æ‹¬ä¼˜åŒ–å¼€é”€ï¼‰
        assert avg_latency < 0.05, f"å¹³å‡å»¶è¿Ÿ {avg_latency:.3f}s è¶…è¿‡é˜ˆå€¼"
        # æœ€å¤§å»¶è¿Ÿåº”è¯¥å°äº100ms
        assert max_latency < 0.1, f"æœ€å¤§å»¶è¿Ÿ {max_latency:.3f}s è¶…è¿‡é˜ˆå€¼"
        
        print(f"âœ… å¯ç”¨ä¼˜åŒ–å - å¹³å‡å»¶è¿Ÿ: {avg_latency:.3f}s, æœ€å¤§å»¶è¿Ÿ: {max_latency:.3f}s")
    
    async def test_chat_latency_without_optimization(self, service_config_no_optimization):
        """æµ‹è¯•æœªå¯ç”¨ä¼˜åŒ–æ—¶çš„èŠå¤©å»¶è¿Ÿï¼ˆåŸºå‡†ï¼‰"""
        # Arrange
        service = LLMService(service_config_no_optimization)
        await service.initialize()
        
        adapter = MockAdapter(delay=0.01)
        service.register_adapter(adapter)
        
        messages = [{"role": "user", "content": "Hello"}]
        
        # Act - æ‰§è¡Œå¤šæ¬¡è¯·æ±‚å¹¶æµ‹é‡å»¶è¿Ÿ
        latencies = []
        for _ in range(10):
            start_time = time.time()
            await service.chat(messages)
            latency = time.time() - start_time
            latencies.append(latency)
        
        # Assert
        avg_latency = sum(latencies) / len(latencies)
        max_latency = max(latencies)
        
        print(f"ğŸ“Š æœªå¯ç”¨ä¼˜åŒ– - å¹³å‡å»¶è¿Ÿ: {avg_latency:.3f}s, æœ€å¤§å»¶è¿Ÿ: {max_latency:.3f}s")
    
    async def test_cache_performance(self, service_config):
        """æµ‹è¯•ç¼“å­˜æ€§èƒ½æå‡"""
        # Arrange
        service = LLMService(service_config)
        await service.initialize()
        
        adapter = MockAdapter(delay=0.05)  # è¾ƒå¤§çš„å»¶è¿Ÿä»¥çªå‡ºç¼“å­˜æ•ˆæœ
        service.register_adapter(adapter)
        
        messages = [{"role": "user", "content": "Hello"}]
        
        # Act - ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼ˆæ— ç¼“å­˜ï¼‰
        start_time = time.time()
        await service.chat(messages)
        first_request_time = time.time() - start_time
        
        # ç¬¬äºŒæ¬¡è¯·æ±‚ï¼ˆæœ‰ç¼“å­˜ï¼‰
        start_time = time.time()
        await service.chat(messages)
        second_request_time = time.time() - start_time
        
        # Assert
        # ç¼“å­˜è¯·æ±‚åº”è¯¥æ˜æ˜¾æ›´å¿«
        assert second_request_time < first_request_time * 0.5, \
            f"ç¼“å­˜æœªç”Ÿæ•ˆ: ç¬¬ä¸€æ¬¡ {first_request_time:.3f}s, ç¬¬äºŒæ¬¡ {second_request_time:.3f}s"
        
        print(f"âœ… ç¼“å­˜æ€§èƒ½ - ç¬¬ä¸€æ¬¡: {first_request_time:.3f}s, ç¬¬äºŒæ¬¡: {second_request_time:.3f}s")
    
    async def test_concurrent_requests(self, service_config):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½"""
        # Arrange
        service = LLMService(service_config)
        await service.initialize()
        
        adapter = MockAdapter(delay=0.01)
        service.register_adapter(adapter)
        
        messages = [{"role": "user", "content": "Hello"}]
        
        # Act - å¹¶å‘æ‰§è¡Œå¤šä¸ªè¯·æ±‚
        start_time = time.time()
        tasks = [service.chat(messages) for _ in range(20)]
        await asyncio.gather(*tasks)
        total_time = time.time() - start_time
        
        # Assert
        # å¹¶å‘è¯·æ±‚æ€»æ—¶é—´åº”è¯¥è¿œå°äºä¸²è¡Œè¯·æ±‚æ—¶é—´
        # ä¸²è¡Œæ—¶é—´çº¦ä¸º 20 * 0.01 = 0.2sï¼Œå¹¶å‘åº”è¯¥æ˜æ˜¾æ›´å¿«
        assert total_time < 0.15, f"å¹¶å‘æ€§èƒ½ä¸ä½³: æ€»æ—¶é—´ {total_time:.3f}s"
        
        print(f"âœ… å¹¶å‘æ€§èƒ½ - 20ä¸ªè¯·æ±‚æ€»æ—¶é—´: {total_time:.3f}s")
    
    async def test_stream_chat_latency(self, service_config):
        """æµ‹è¯•æµå¼èŠå¤©å»¶è¿Ÿ"""
        # Arrange
        service = LLMService(service_config)
        await service.initialize()
        
        adapter = MockAdapter(delay=0.01)
        service.register_adapter(adapter)
        
        messages = [{"role": "user", "content": "Hello"}]
        
        # Act - æµ‹é‡é¦–å—å»¶è¿Ÿ
        start_time = time.time()
        first_chunk = None
        async for chunk in service.stream_chat(messages):
            if first_chunk is None:
                first_chunk_time = time.time() - start_time
                first_chunk = chunk
                break
        
        # Assert
        # é¦–å—å»¶è¿Ÿåº”è¯¥å°äº50ms
        assert first_chunk_time < 0.05, f"é¦–å—å»¶è¿Ÿ {first_chunk_time:.3f}s è¶…è¿‡é˜ˆå€¼"
        
        print(f"âœ… æµå¼å“åº”é¦–å—å»¶è¿Ÿ: {first_chunk_time:.3f}s")
    
    async def test_memory_usage(self, service_config):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import psutil
        import os
        
        # Arrange
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        service = LLMService(service_config)
        await service.initialize()
        
        adapter = MockAdapter()
        service.register_adapter(adapter)
        
        messages = [{"role": "user", "content": "Hello"}]
        
        # Act - æ‰§è¡Œå¤šæ¬¡è¯·æ±‚
        for _ in range(100):
            await service.chat(messages)
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        # Assert
        # å†…å­˜å¢é•¿åº”è¯¥å°äº50MBï¼ˆåŒ…æ‹¬ç¼“å­˜ç­‰ï¼‰
        assert memory_increase < 50, f"å†…å­˜å¢é•¿ {memory_increase:.2f}MB è¶…è¿‡é˜ˆå€¼"
        
        print(f"âœ… å†…å­˜ä½¿ç”¨ - åˆå§‹: {initial_memory:.2f}MB, æœ€ç»ˆ: {final_memory:.2f}MB, å¢é•¿: {memory_increase:.2f}MB")
    
    async def test_cleanup(self, service_config):
        """æµ‹è¯•èµ„æºæ¸…ç†"""
        # Arrange
        service = LLMService(service_config)
        await service.initialize()
        
        # Act
        await service.cleanup()
        
        # Assert - åº”è¯¥æ²¡æœ‰å¼‚å¸¸æŠ›å‡º
        assert True
